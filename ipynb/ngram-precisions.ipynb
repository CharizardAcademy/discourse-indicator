{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d14b9482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yingqiang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/yingqiang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yingqiang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import sent2vec\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import  RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from functools import lru_cache\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from sqlitedict import SqliteDict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import string\n",
    "\n",
    "import collections\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import faiss\n",
    "\n",
    "from rouge_score import scoring\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.interpolate as interp\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9727b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "scinf_full_text_sections = SqliteDict('../SQLite/scinf-biomed-body-text-sections.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84ebbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scinf_global_pairs = SqliteDict('../SQLite/scinf-biomed-global-arguments.sqlite', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24deb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "scinf_local_pairs_indicator = SqliteDict('../SQLite/scinf-biomed-LAC-indicator.sqlite.sqlite', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be39bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../SQLite/paper_ids.json', 'r') as f:\n",
    "    paper_ids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565a70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_strings = ['method', 'procedure', 'data', 'theory', 'theorem', 'implementation']\n",
    "result_strings = ['result', 'outcome', 'analysis', 'measure', 'evaluation']\n",
    "conclusion_strings = ['conclusion', 'summary', 'concluding', 'remark', 'key point']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98d24b",
   "metadata": {},
   "source": [
    "### unigram precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77e88d",
   "metadata": {},
   "source": [
    "#### premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98973205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27924/27924 [24:37<00:00, 18.90it/s]  \n"
     ]
    }
   ],
   "source": [
    "section_pre_precisions = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "section_pre_lengths = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "\n",
    "for key in tqdm(paper_ids):\n",
    "    \n",
    "    try:\n",
    "        local_premises = scinf_local_pairs_indicator[key]['premise']\n",
    "        global_premises = scinf_global_pairs[key]['premise']\n",
    "\n",
    "        local_front = scinf_local_pairs_indicator[key]['front']\n",
    "        local_back = scinf_local_pairs_indicator[key]['back']\n",
    "\n",
    "        sections = scinf_full_text_sections[key]['body_text']\n",
    "        \n",
    "        for section in sections:\n",
    "            for pre in local_premises:\n",
    "                if pre and pre in section['text']:\n",
    "                    pre_common_words = list(set(word_tokenize(global_premises)) & set(word_tokenize(\"\\n\".join(pre) + \"\\n\".join(local_front) + \"\\n\".join(local_back))))\n",
    "                    pre_common_words = [word.lower() for word in pre_common_words if word.lower() not in set(stopwords.words('english')) and word not in string.punctuation]\n",
    "                    \n",
    "                    local_pre_no_punc = [word.lower() for word in set(word_tokenize(\"\\n\".join(pre) + \"\\n\".join(local_front) + \"\\n\".join(local_back))) if word not in string.punctuation]\n",
    "                    local_pre_no_punc = [word for word in local_pre_no_punc if word not in stopwords.words('english')]\n",
    "                    pre_precision = len(pre_common_words) / len(local_pre_no_punc)\n",
    "\n",
    "                    section_length_in_words = len(word_tokenize(section['text']))\n",
    "\n",
    "                    if any(section_name in ''.join(section['section']).lower() for section_name in method_strings):\n",
    "                        section_pre_precisions['method'].append(pre_precision)\n",
    "                        section_pre_lengths['method'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in result_strings):\n",
    "                        section_pre_precisions['result'].append(pre_precision)\n",
    "                        section_pre_lengths['result'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in conclusion_strings):\n",
    "                        section_pre_precisions['conclusion'].append(pre_precision)\n",
    "                        section_pre_lengths['conclusion'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    else:\n",
    "                        section_pre_precisions['other'].append(pre_precision)\n",
    "                        section_pre_lengths['other'].append(section_length_in_words)\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606739a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: precision 0.11449390562055678, std. 0.06350609961608047\n",
      "result: precision 0.12828548288278405, std. 0.07142852547911879\n",
      "conclusion: precision 0.12222056075190889, std. 0.0691167533556677\n",
      "other: precision 0.11568171975723499, std. 0.061870236975449674\n"
     ]
    }
   ],
   "source": [
    "print(f\"method: precision {np.mean(section_pre_precisions['method'])}, std. {np.std(section_pre_precisions['method'])}\")\n",
    "print(f\"result: precision {np.mean(section_pre_precisions['result'])}, std. {np.std(section_pre_precisions['result'])}\")\n",
    "print(f\"conclusion: precision {np.mean(section_pre_precisions['conclusion'])}, std. {np.std(section_pre_precisions['conclusion'])}\")\n",
    "print(f\"other: precision {np.mean(section_pre_precisions['other'])}, std. {np.std(section_pre_precisions['other'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8980328",
   "metadata": {},
   "source": [
    "#### conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4db4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27924/27924 [24:46<00:00, 18.78it/s] \n"
     ]
    }
   ],
   "source": [
    "section_con_precisions = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "section_con_lengths = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "\n",
    "for key in tqdm(paper_ids):\n",
    "    #\n",
    "    try:\n",
    "        local_conclusions = scinf_local_pairs_indicator[key]['conclusion']\n",
    "        global_conclusions = scinf_global_pairs[key]['conclusion']\n",
    "\n",
    "        local_front = scinf_local_pairs_indicator[key]['front']\n",
    "        local_back = scinf_local_pairs_indicator[key]['back']\n",
    "\n",
    "        sections = scinf_full_text_sections[key]['body_text']\n",
    "        \n",
    "        for section in sections:\n",
    "            for con in local_conclusions:\n",
    "                if con and con in section['text']:\n",
    "                    con_common_words = list(set(word_tokenize(global_conclusions)) & set(word_tokenize(\"\\n\".join(con) + \"\\n\".join(local_front) + \"\\n\".join(local_back))))\n",
    "                    con_common_words = [word.lower() for word in con_common_words if word.lower() not in set(stopwords.words('english')) and word not in string.punctuation]\n",
    "                    \n",
    "                    #if len(con_common_words) != 0:\n",
    "                    local_con_no_punc = [word.lower() for word in set(word_tokenize(\"\\n\".join(con) + \"\\n\".join(local_front) + \"\\n\".join(local_back))) if word not in string.punctuation]\n",
    "                    local_con_no_punc = [word for word in local_con_no_punc if word not in stopwords.words('english')]\n",
    "                    con_precision = len(con_common_words) / len(local_con_no_punc)\n",
    "                    #else:\n",
    "                        #continue\n",
    "\n",
    "                    section_length_in_words = len(word_tokenize(section['text']))\n",
    "\n",
    "                    if any(section_name in ''.join(section['section']).lower() for section_name in method_strings):\n",
    "                        section_con_precisions['method'].append(con_precision)\n",
    "                        section_con_lengths['method'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in result_strings):\n",
    "                        section_con_precisions['result'].append(con_precision)\n",
    "                        section_con_lengths['result'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in conclusion_strings):\n",
    "                        section_con_precisions['conclusion'].append(con_precision)\n",
    "                        section_con_lengths['conclusion'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    else:\n",
    "                        section_con_precisions['other'].append(con_precision)\n",
    "                        section_con_lengths['other'].append(section_length_in_words)\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828f7410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: precision 0.08114209798249236, std. 0.05229816307229417\n",
      "result: precision 0.08512990396895426, std. 0.05471712854954438\n",
      "conclusion: precision 0.10442115014288407, std. 0.06674194467369407\n",
      "other: precision 0.08624275965657859, std. 0.05154516382528145\n"
     ]
    }
   ],
   "source": [
    "print(f\"method: precision {np.mean(section_con_precisions['method'])}, std. {np.std(section_con_precisions['method'])}\")\n",
    "print(f\"result: precision {np.mean(section_con_precisions['result'])}, std. {np.std(section_con_precisions['result'])}\")\n",
    "print(f\"conclusion: precision {np.mean(section_con_precisions['conclusion'])}, std. {np.std(section_con_precisions['conclusion'])}\")\n",
    "print(f\"other: precision {np.mean(section_con_precisions['other'])}, std. {np.std(section_con_precisions['other'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f7420",
   "metadata": {},
   "source": [
    "### bigram precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157bf1c",
   "metadata": {},
   "source": [
    "#### conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3309e30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27924/27924 [04:31<00:00, 102.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute precisions of bigrams in related sections\n",
    "\n",
    "section_con_precisions = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "section_con_lengths = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "\n",
    "for key in tqdm(paper_ids):\n",
    "    #\n",
    "    try:\n",
    "        local_conclusions = scinf_local_pairs_indicator[key]['conclusion']\n",
    "        global_conclusions = scinf_global_pairs[key]['conclusion']\n",
    "\n",
    "        local_front = scinf_local_pairs_indicator[key]['front']\n",
    "        local_back = scinf_local_pairs_indicator[key]['back']\n",
    "\n",
    "        sections = scinf_full_text_sections[key]['body_text']\n",
    "\n",
    "        for section in sections:\n",
    "            for con in local_conclusions:\n",
    "                if con and con in section['text']:\n",
    "                    \n",
    "                    global_con_bigrams = nltk.bigrams(word_tokenize(global_conclusions))\n",
    "                    local_con_bigrams = nltk.bigrams(word_tokenize(\"\\n\".join(con) + \"\\n\".join(local_front) + \"\\n\".join(local_back)))\n",
    "        \n",
    "                    global_con_bigrams = [bigram for bigram in global_con_bigrams]\n",
    "                    local_con_bigrams = [bigram for bigram in local_con_bigrams]\n",
    "        \n",
    "                    con_common_bigrams = list(set(global_con_bigrams) & set(local_con_bigrams))\n",
    "        \n",
    "                    #if len(con_common_bigrams) != 0:\n",
    "                    con_precision = len(con_common_bigrams) / len(local_con_bigrams)\n",
    "                    #else:\n",
    "                        #continue\n",
    "            \n",
    "                    section_length_in_words = len(word_tokenize(section['text']))\n",
    "                    \n",
    "                    if any(section_name in ''.join(section['section']).lower() for section_name in method_strings):\n",
    "                        section_con_precisions['method'].append(con_precision)\n",
    "                        section_con_lengths['method'].append(section_length_in_words)\n",
    "                        continue\n",
    "\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in result_strings):\n",
    "                        section_con_precisions['result'].append(con_precision)\n",
    "                        section_con_lengths['result'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in conclusion_strings):\n",
    "                        section_con_precisions['conclusion'].append(con_precision)\n",
    "                        section_con_lengths['conclusion'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    else:\n",
    "                        section_con_precisions['other'].append(con_precision)\n",
    "                        section_con_lengths['other'].append(section_length_in_words)\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a88992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: precision 0.021565040712589172, std. 0.027008427180399145\n",
      "result: precision 0.023301509272271297, std. 0.029353950676067452\n",
      "conclusion: precision 0.0302953680406899, std. 0.03657745762761869\n",
      "other: precision 0.02203967592564201, std. 0.02530390036067794\n"
     ]
    }
   ],
   "source": [
    "print(f\"method: precision {np.mean(section_con_precisions['method'])}, std. {np.std(section_con_precisions['method'])}\")\n",
    "print(f\"result: precision {np.mean(section_con_precisions['result'])}, std. {np.std(section_con_precisions['result'])}\")\n",
    "print(f\"conclusion: precision {np.mean(section_con_precisions['conclusion'])}, std. {np.std(section_con_precisions['conclusion'])}\")\n",
    "print(f\"other: precision {np.mean(section_con_precisions['other'])}, std. {np.std(section_con_precisions['other'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaed956",
   "metadata": {},
   "source": [
    "#### premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26bc038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27924/27924 [04:22<00:00, 106.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute precisions of bigrams in related sections\n",
    "\n",
    "section_pre_precisions = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "section_pre_lengths = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "\n",
    "for key in tqdm(paper_ids):\n",
    "    #\n",
    "    try:\n",
    "        local_premises = scinf_local_pairs_indicator[key]['premise']\n",
    "        global_premises = scinf_global_pairs[key]['premise']\n",
    "\n",
    "        local_front = scinf_local_pairs_indicator[key]['front']\n",
    "        local_back = scinf_local_pairs_indicator[key]['back']\n",
    "\n",
    "        sections = scinf_full_text_sections[key]['body_text']\n",
    "\n",
    "        for section in sections:\n",
    "            for pre in local_premises:\n",
    "                if pre and pre in section['text']:\n",
    "                    \n",
    "                    global_pre_bigrams = nltk.bigrams(word_tokenize(global_premises))\n",
    "                    local_pre_bigrams = nltk.bigrams(word_tokenize(\"\\n\".join(pre) + \"\\n\".join(local_front) + \"\\n\".join(local_back)))\n",
    "        \n",
    "                    global_pre_bigrams = [bigram for bigram in global_pre_bigrams]\n",
    "                    local_pre_bigrams = [bigram for bigram in local_pre_bigrams]\n",
    "        \n",
    "                    pre_common_bigrams = list(set(global_pre_bigrams) & set(local_pre_bigrams))\n",
    "        \n",
    "                    #if len(local_pre_bigrams) != 0:\n",
    "                    pre_precision = len(pre_common_bigrams) / len(local_pre_bigrams)\n",
    "                    \n",
    "                    section_length_in_words = len(word_tokenize(section['text']))\n",
    "                    \n",
    "                    if any(section_name in ''.join(section['section']).lower() for section_name in method_strings):\n",
    "                        section_pre_precisions['method'].append(pre_precision)\n",
    "                        section_pre_lengths['method'].append(section_length_in_words)\n",
    "                        continue\n",
    "\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in result_strings):\n",
    "                        section_pre_precisions['result'].append(pre_precision)\n",
    "                        section_pre_lengths['result'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in conclusion_strings):\n",
    "                        section_pre_precisions['conclusion'].append(pre_precision)\n",
    "                        section_pre_lengths['conclusion'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    else:\n",
    "                        section_pre_precisions['other'].append(pre_precision)\n",
    "                        section_pre_lengths['other'].append(section_length_in_words)\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ea1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: precision 0.030426325150739456, std. 0.029171707873956922\n",
      "result: precision 0.035869280463603236, std. 0.03429453955434652\n",
      "conclusion: precision 0.03321985859806792, std. 0.03611131788516471\n",
      "other: precision 0.02955333565264894, std. 0.027970106894858123\n"
     ]
    }
   ],
   "source": [
    "print(f\"method: precision {np.mean(section_pre_precisions['method'])}, std. {np.std(section_pre_precisions['method'])}\")\n",
    "print(f\"result: precision {np.mean(section_pre_precisions['result'])}, std. {np.std(section_pre_precisions['result'])}\")\n",
    "print(f\"conclusion: precision {np.mean(section_pre_precisions['conclusion'])}, std. {np.std(section_pre_precisions['conclusion'])}\")\n",
    "print(f\"other: precision {np.mean(section_pre_precisions['other'])}, std. {np.std(section_pre_precisions['other'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e425d2",
   "metadata": {},
   "source": [
    "### trigram precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e51869",
   "metadata": {},
   "source": [
    "#### conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427ea329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27924/27924 [04:28<00:00, 103.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute precisions of trigrams in related sections\n",
    "\n",
    "section_con_precisions = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "section_con_lengths = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "\n",
    "for key in tqdm(paper_ids):\n",
    "    #\n",
    "    try:\n",
    "        local_conclusions = scinf_local_pairs_indicator[key]['conclusion']\n",
    "        global_conclusions = scinf_global_pairs[key]['conclusion']\n",
    "\n",
    "        local_front = scinf_local_pairs_indicator[key]['front']\n",
    "        local_back = scinf_local_pairs_indicator[key]['back']\n",
    "\n",
    "        sections = scinf_full_text_sections[key]['body_text']\n",
    "\n",
    "        for section in sections:\n",
    "            for con in local_conclusions:\n",
    "                if con and con in section['text']:\n",
    "                    global_con_trigrams = nltk.ngrams(word_tokenize(global_conclusions), 3)\n",
    "                    local_con_trigrams = nltk.ngrams(word_tokenize(\"\\n\".join(con) + \"\\n\".join(local_front) + \"\\n\".join(local_back)), 3)\n",
    "        \n",
    "                    global_con_trigrams = [trigram for trigram in global_con_trigrams]\n",
    "                    local_con_trigrams = [trigram for trigram in local_con_trigrams]\n",
    "        \n",
    "                    con_common_trigrams = list(set(global_con_trigrams) & set(local_con_trigrams))\n",
    "        \n",
    "                    #if len(con_common_trigrams) >= 5:\n",
    "                    con_precision = len(con_common_trigrams) / len(local_con_trigrams)\n",
    "            \n",
    "                    section_length_in_words = len(word_tokenize(section['text']))\n",
    "                    \n",
    "                    if any(section_name in ''.join(section['section']).lower() for section_name in method_strings):\n",
    "                        section_con_precisions['method'].append(con_precision)\n",
    "                        section_con_lengths['method'].append(section_length_in_words)\n",
    "                        continue\n",
    "\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in result_strings):\n",
    "                        section_con_precisions['result'].append(con_precision)\n",
    "                        section_con_lengths['result'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in conclusion_strings):\n",
    "                        section_con_precisions['conclusion'].append(con_precision)\n",
    "                        section_con_lengths['conclusion'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    else:\n",
    "                        section_con_precisions['other'].append(con_precision)\n",
    "                        section_con_lengths['other'].append(section_length_in_words)\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b47f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: precision 0.008853747902477243, std. 0.02214665310223368\n",
      "result: precision 0.010098362322544538, std. 0.025127175236043926\n",
      "conclusion: precision 0.016886950630968307, std. 0.03347478225399961\n",
      "other: precision 0.009301942823263857, std. 0.020883408764756042\n"
     ]
    }
   ],
   "source": [
    "print(f\"method: precision {np.mean(section_con_precisions['method'])}, std. {np.std(section_con_precisions['method'])}\")\n",
    "print(f\"result: precision {np.mean(section_con_precisions['result'])}, std. {np.std(section_con_precisions['result'])}\")\n",
    "print(f\"conclusion: precision {np.mean(section_con_precisions['conclusion'])}, std. {np.std(section_con_precisions['conclusion'])}\")\n",
    "print(f\"other: precision {np.mean(section_con_precisions['other'])}, std. {np.std(section_con_precisions['other'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0531ba",
   "metadata": {},
   "source": [
    "#### premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27fec69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27924/27924 [04:21<00:00, 106.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute precisions of trigrams in related sections\n",
    "\n",
    "section_pre_precisions = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "section_pre_lengths = {\"method\":list(), \"result\":list(), \"conclusion\":list(), \"other\":list()}\n",
    "\n",
    "for key in tqdm(paper_ids):\n",
    "    #\n",
    "    try:\n",
    "        local_premises = scinf_local_pairs_indicator[key]['premise']\n",
    "        global_premises = scinf_global_pairs[key]['premise']\n",
    "\n",
    "        local_front = scinf_local_pairs_indicator[key]['front']\n",
    "        local_back = scinf_local_pairs_indicator[key]['back']\n",
    "\n",
    "        sections = scinf_full_text_sections[key]['body_text']\n",
    "\n",
    "        \n",
    "        for section in sections:\n",
    "            for pre in local_premises:    \n",
    "                if pre and pre in section['text']:\n",
    "                    global_pre_trigrams = nltk.ngrams(word_tokenize(global_premises), 3)\n",
    "                    local_pre_trigrams = nltk.ngrams(word_tokenize(\"\\n\".join(pre) + \"\\n\".join(local_front) + \"\\n\".join(local_back)), 3)\n",
    "        \n",
    "                    global_pre_trigrams = [trigram for trigram in global_pre_trigrams]\n",
    "                    local_pre_trigrams = [trigram for trigram in local_pre_trigrams]\n",
    "        \n",
    "                    pre_common_trigrams = list(set(global_pre_trigrams) & set(local_pre_trigrams))\n",
    "        \n",
    "                    #if len(local_pre_trigrams) != 0:\n",
    "                    pre_precision = len(pre_common_trigrams) / len(local_pre_trigrams)\n",
    "                    #else:\n",
    "                        #continue\n",
    "            \n",
    "                    section_length_in_words = len(word_tokenize(section['text']))\n",
    "                    \n",
    "                    if any(section_name in ''.join(section['section']).lower() for section_name in method_strings):\n",
    "                        section_pre_precisions['method'].append(pre_precision)\n",
    "                        section_pre_lengths['method'].append(section_length_in_words)\n",
    "                        continue\n",
    "\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in result_strings):\n",
    "                        section_pre_precisions['result'].append(pre_precision)\n",
    "                        section_pre_lengths['result'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    elif any(section_name in ''.join(section['section']).lower() for section_name in conclusion_strings):\n",
    "                        section_pre_precisions['conclusion'].append(pre_precision)\n",
    "                        section_pre_lengths['conclusion'].append(section_length_in_words)\n",
    "                        continue\n",
    "                    else:\n",
    "                        section_pre_precisions['other'].append(pre_precision)\n",
    "                        section_pre_lengths['other'].append(section_length_in_words)\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56027f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: precision 0.010974766931433738, std. 0.020821531604121705\n",
      "result: precision 0.013895545278966803, std. 0.02668431670893354\n",
      "conclusion: precision 0.013490445610424985, std. 0.029228161405617473\n",
      "other: precision 0.010520794733471365, std. 0.0199192219535281\n"
     ]
    }
   ],
   "source": [
    "print(f\"method: precision {np.mean(section_pre_precisions['method'])}, std. {np.std(section_pre_precisions['method'])}\")\n",
    "print(f\"result: precision {np.mean(section_pre_precisions['result'])}, std. {np.std(section_pre_precisions['result'])}\")\n",
    "print(f\"conclusion: precision {np.mean(section_pre_precisions['conclusion'])}, std. {np.std(section_pre_precisions['conclusion'])}\")\n",
    "print(f\"other: precision {np.mean(section_pre_precisions['other'])}, std. {np.std(section_pre_precisions['other'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
